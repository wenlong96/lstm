---
title: "LSTM: A Deep Learning Approach to Multivariate Time Series Forecasting"
output: github_document
always_allow_html: true
---

In a landscape where established machine learning techniques based on Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost) and classical methods based on ARIMA (e.g., SARIMA, ARIMA-X) are far and wide, deep learning opens up a new realm of possibilities for time series forecasting. In this tutorial, we will explore how Recurrent Neural Networks (RNN), particularly the powerful Long Short-Term Memory (LSTM) architecture, can be used for precise predictions when dealing with time series data. LSTM networks are a type of RNN architecture designed to address some of the limitations of traditional RNNs, such as the vanishing gradient problem, and to capture long-range dependencies in time series data.


Please note that this tutorial serves as a foundational introduction to implementing a fundamental multivariate LSTM framework. While it offers a primer on working with multivariate time series data, it's important to recognize that when grappling with intricate high-dimensional temporal data or multiple concurrent time series, a more sophisticated approach might be in order, such as Amazon's DeepAR. While not making direct predictions via LSTMs, DeepAR employs the underlying power of LSTMs to parameterize a Gaussian likelihood function and is able to capture complex and group-dependent relationships by using covariates. For more information on how the LSTM architecture and DeepAR work, here are some materials I recommend:

Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735

Peixeiro, M. (2022). Time series forecasting in Python ([First edition].). Manning Publications Co.

Salinas, D., Flunkert, V., Gasthaus, J., & Januschowski, T. (2020). DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3), 1181–1191. https://doi.org/10.1016/j.ijforecast.2019.07.001


For this tutorial, we will be using the dataset provided by Kaggle in their 2018 competition "Store Item Demand Forecasting Challenge" (https://www.kaggle.com/competitions/demand-forecasting-kernels-only).

We will first start with data preprocessing.

# Data preprocessing
```{r}
df <- read.csv("C://Users//wl//Desktop//Work//[Website]//Demand Forecast//Kaggle Challenge//train.csv")
View(df) # 2013-01-01 to 2017-12-31

unique(df$store)
unique(df$item)

df[!complete.cases(df), ] # find how many NA rows
```

Upon examination, we discover that the dataset encompasses records spanning from January 1, 2013, to December 31, 2017. Within this data, we have information from 10 distinct stores and 50 unique items.


Unlike more advanced and complex approaches like DeepAR, LSTM is only suitable for single time series data. For the purpose of this tutorial, we will focus on the data obtained for item 1 from store 1. We will also do some exploratory data analysis (EDA) to delve deeper into the dataset.

# Exploratory data analysis
```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(scales)
library(ggthemes)

df2 <- df |>
  filter(store == 1 & item == 1) |>
  arrange(date)

df2$date <- ymd(df2$date) # convert to date format

pacf(df2$sales) # we can see that day-of-the-week effect and the previous-day effect are the most important

exploratory <- ggplot() +
  geom_line(data = df2, aes(x = date, y = sales, color = "#CC79A7")) +
  scale_x_date(labels = date_format("%Y-%b"), breaks = date_breaks("12 months")) +
  xlab("Date") +
  ylab("Amount ($)") +
  ggtitle("Daily Sales for item 1 store 1") +
  guides(color = "none")

exploratory
```

Evidently, the sales figure shows a recurring seasonality pattern, accompanied by a subtle ascending trend across time. Furthermore, both the day-of-the-week and the previous-day's effects seems to be noteworthy factors. Because of LSTM's structure, it will be able to capture these temporal relationships.


We will now proceed with feature engineering to add crucial features that will help us forecast our target variable. Incorporating US Personal Consumption Expenditures (PCE) information from the FRED database (https://fred.stlouisfed.org/series/PCE) adds an extra layer of feature to our analysis. While the dataset lack explicit geographical markers, we assume that these figures are rooted in US-specific data given that the data is disseminated through Kaggle and considering that Kaggle's headquarters are US-based. There are also many other features we can include in our model, but some of these may or may not be applicable depending on context and the results from the time series decomposition we observed, as well as from PACF plots. Nevertheless, some of these features are listed below.

Seasonality: We will use the already extracted seasonality in training our model. Seasonality captures recurring patterns that occur at specific times within each year.

Month and Day Variables: We will use separate variables for month and day to capture any monthly or daily patterns in the data.

Day of the Week: Including a variable for the day of the week will help us differentiate sales patterns between weekdays and weekends, as they are likely to differ.

Day of the Year: Adding a variable for the day of the year will help us identify any annual trends or patterns.

Date Components: We will include variables to indicate whether it is a weekend, the start/end of the month, and the start/end of the year. These date components can reveal specific patterns related to these time frames.

Lagged Periods: Including lagged values of the target variable can help us capture any dependencies or autocorrelation within the time series data.

Running Days Column: We will create a column that captures the number of days elapsed since the start of the time series data. This will help us model the trend component over time.

Public Holidays: If we have information about public holidays, we can include them as features. Public holidays can have a significant impact on sales patterns.

Restock Inventory Information: If available, including information about restock inventory can help us account for sudden changes in stock levels that may influence sales.

Anomalies in Operations: Including information about anomalies in operations can help us identify and account for any irregularities in the data that may affect sales.

Macroeconomic Factors: If relevant, we can include other macroeconomic factors that may have an impact on sales, such as inflation rates, unemployment rates, or GDP growth.

# Feature engineering
```{r}
pce <- read.csv("C://Users//wl//Desktop//Work//[Website]//Demand Forecast//Kaggle Challenge//PCE.csv")
pce$DATE <- ymd(pce$DATE) # convert to date format

# rename DATE to lowercase and create year and month columns
pce$date <- pce$DATE 
pce <- pce |>
  select(-DATE) |>
  mutate(year = year(date), month = month(date))

# join pce with df2
df3 <- df2 |>
  mutate(year = year(date), month = month(date)) |>
  left_join(pce, by = c("year", "month")) |>
  mutate(date = date.x) |>
  select(date, sales, PCE)
```


We will now proceed with additional data preprocessing steps before constructing our model architecture. We will first apply feature scaling to our dataset. This serves a multifaceted purpose: it facilitates faster convergence during training, optimizes gradient descent algorithms, and enhances the stability of activation functions. While there exist other advantages to scaling prior to modeling, I leave the exploration of these aspects to readers.

Next, we will generate the training and test sets. Due to temporal dependency, it is important to split our time series data in a manner that the test data comprises the most recent periods. Since we are doing a multi-step forecast, the accuracy of the next period's forecast is likely to surpass those made further into the future. This is because the next period's forecast can rely on actual observed lagged values, whereas forecasts further into the future must rely on predicted lagged values. As we forecast further into the future, the number of predictions to be made increases which can lead to compounding errors. Nevertheless, it is still a powerful time series forecasting method and can produce accurate results.

Finally, We will also prepare the three-dimensional arrays here for both input (x) and output (y) data which we will feed into the training process and validate our results.

# Data preprocessing
```{r}
library(keras)
library(tensorflow)

tensorflow::set_random_seed( 
  seed = 333, 
  disable_gpu = FALSE
) 

# scaling the dataset
scale_factors_sales <- c(mean(df3$sales), sd(df3$sales))
scale_factors_consumption <- c(mean(df3$PCE), sd(df3$PCE))

scaled_train <- df3 |>
  mutate(sales = (sales - scale_factors_sales[1]) / scale_factors_sales[2]) |>
  mutate(PCE = (PCE - scale_factors_consumption[1]) / scale_factors_consumption[2]) |>
  select(sales, PCE)


# preparing the three-dimensional arrays
lag <- 364

scaled_train <- as.matrix(scaled_train)
 
x_train_data_1 <- t(sapply(
    1:(nrow(df3) - lag - lag),
    function(x) scaled_train[x:(x + lag - 1), 1]
  ))
 
x_train_data_2 <- t(sapply(
    1:(nrow(df3) - lag - lag),
    function(x) scaled_train[x:(x + lag - 1), 2]
  ))

x_train_arr <- array(
    data = as.numeric(unlist(c(x_train_data_1, x_train_data_2))),
    dim = c(
        nrow(x_train_data_1),
        lag,
        2
    )
)

y_train_data <- scaled_train[(nrow(df3) - nrow(x_train_data_1) + 1 - lag):(nrow(df3) - lag), 1]

y_train_arr <- array(
    data = as.numeric(unlist(c(y_train_data))),
    dim = c(
        length(y_train_data),
        1
    )
)
```


With our data preprocessed and primed for modeling, we will now construct the framework of our neural network. While a comprehensive approach would involve hyperparameter optimization to determine the optimal settings, our current tutorial focuses on establishing the foundational implementation of LSTM. As such, we will craft our own model structure to illustrate the fundamental concepts. In this step, we will define the architecture of our LSTM-based model. This involves designing the number of layers, nodes within each layer, and the sequence of operations that will transform our input data into meaningful predictions. 

We initiate with an LSTM layer of 64 units before incorporating a dropout rate of 0.4 to mitigate overfitting. Then, we apply a bidirectional LSTM layer of 64 units (effectively rendering a 128-unit layer) to enhances the network's ability to capture nuanced temporal patterns. A further layer of dropout at 0.4 will be applied to mitigate overfitting and improve generalization. Finally, we will use a dense component with a single unit to render the predicted outcomes. This architecture consistently leverages on hyperbolic tangents as the activation functions to foster non-linearity within the model's transformations. We will use Mean Squared Error (MSE) as our loss function and Adaptive Moment Estimation (Adam) as our optimizer, which combines the benefits of both the Adagrad and RMSprop optimizers.

# Constructing model architecture
```{r}
# initiating a keras model
lstm_model <- keras_model_sequential()

# adding layers and compiling the model
lstm_model |>
  layer_lstm(
    units = 64,
    batch_input_shape = c(1, lag, 2), # batch size, timesteps, features
    return_sequences = TRUE,
    stateful = TRUE,
    activation = "tanh"
  ) |>
  layer_dropout(rate = 0.4) |>
  bidirectional(layer_lstm(units = 64)) |>
  layer_dropout(rate = 0.4) |>
  layer_dense(units = 1, activation = "tanh") |>
  compile(loss = 'mean_squared_error',
          optimizer = "adam",
          metrics = 'mse')

summary(lstm_model)
```


We are now finally ready to train our model. We will be training 10 epochs with a validation split of 0.2, and we will also employ an early stopping rule based on validation loss if results for 2 subsequent epochs stops improving.

# Training the model
```{r}
# fitting the model to training dataset
history <- lstm_model |> fit(
  x = x_train_arr,
  y = y_train_arr,
  batch_size = 1,
  epochs = 10,
  verbose = 1,
  shuffle = FALSE,
  use_multiprocessing = T,
  validation_split = 0.2,
  callback_early_stopping(
    monitor = "val_loss",
    min_delta = 0,
    patience = 2,
    verbose = 0,
    mode = "auto",
    baseline = NULL,
    restore_best_weights = TRUE
  )
)

# plotting train and validation loss for each epoch
plot(history)
```

As we can see, there is a close alignment between training and validation loss. This typically suggests that the model is learning effectively and generalizing well to unseen data.


We can now check our model performance against the test data. Due to our simulation of real-world applications, we encounter the challenge that lagged values are only observed for the very next period prediction, and unobserved lagged values increase as we forecast further into the future. To overcome this limitation, we will be using multi-step forecasting to derive these unobservable values which requires an iterative process.

We will also assume consumption values for the testing year to be the same as the values from the corresponding day and month of the previous year which is often referred to as a "naive" or "seasonal naive" forecast. This approach relies on the assumption that consumption patterns have a certain degree of seasonality, and the consumption on a particular day and month tends to be similar from year to year.

# Checking model performance on test data
```{r}
# initialize vector to store forecasts
periods_to_pred <- 364
forecasted_store <- c()

# we will first forecast the very next period with observed lagged values
observed_sequence_1 <- scaled_train[(nrow(df3) - periods_to_pred + 1 - periods_to_pred):(nrow(df3) - periods_to_pred), 1]
observed_sequence_2 <- scaled_train[(nrow(df3) - periods_to_pred + 1 - periods_to_pred):(nrow(df3) - periods_to_pred), 2]

input_sequence <- array(
    data = as.numeric(unlist(c(observed_sequence_1, observed_sequence_2))),
    dim = c(
        1,
        periods_to_pred,
        2
    )
)

forecasted_value <- lstm_model |>
    predict(input_sequence)

forecasted_store <- forecasted_store |>
  append(forecasted_value)

# now we do for the rest of the days in the year
for (i in 1:(periods_to_pred - 1)) {

observed_sequence_1 <- scaled_train[(nrow(df3) - periods_to_pred + 1 + i - periods_to_pred):(nrow(df3) - periods_to_pred), 1]

observed_sequence_1 <- observed_sequence_1 |>
  append(forecasted_store)

observed_sequence_2 <- c(observed_sequence_2[-seq_len(1)], observed_sequence_2[seq_len(1)])  

input_sequence <- array(
    data = as.numeric(unlist(c(observed_sequence_1, observed_sequence_2))),
    dim = c(
        1,
        periods_to_pred,
        2
    )
)

forecasted_value <- lstm_model |>
    predict(input_sequence)

forecasted_store <- forecasted_store |>
  append(forecasted_value)
}
 
# reversing the scaling
forecasted_store2 <- forecasted_store * scale_factors_sales[2] + scale_factors_sales[1]

# creating a dataframe to store results
results <- data.frame(date = df3[(nrow(df3) - periods_to_pred + 1):(nrow(df3)), 1],
                      sales = df3[(nrow(df3) - periods_to_pred + 1):(nrow(df3)), 2],
                      forecast = forecasted_store2)

# summzarizing the accuracy
accuracy <- results |>
  mutate(MAPE_Inv = (results$sales - results$forecast) / results$sales) |>
  mutate(MAPE_Inv_abs = abs(MAPE_Inv)) |>
  summarize(MAPE_accuracy = 1 - mean(MAPE_Inv_abs))

paste0("The average MAPE accuracy is ", accuracy[1, 1])
```


# Plot to show predicted sales vs actual sales figure for test set

```{r}
forecast_analysis <- ggplot() +
  geom_line(data = results, aes(x = as.Date(date), y = sales, color = "Actual"), alpha = 0.35) +
  geom_line(data = results, aes(x = as.Date(date), y = forecast, color = "Forecast")) +
  scale_x_date(labels = date_format("%Y-%b"), breaks = date_breaks("2 months")) +
  xlab("Date") +
  ylab("Amount ($)") +
  ggtitle("Forecasted Sales vs Actual Sales") +
  scale_color_manual(values = c("Actual" = "#009E73", "Forecast" = "#CC79A7"), name = "Legend")

forecast_analysis
```

As evident from the results, the model demonstrates commendable performance across the entire year and seem to capture seasonality and trend well with an accuracy score of 77.6%.

This achievement is particularly remarkable considering the utilization of a multi-step forecasting technique, which inherently introduces compounding errors. Despite this challenge, the model's resilience and ability to maintain decent accuracy levels are evident, which stands as a testament to the inherent strengths of the LSTM architecture.


This tutorial explored the application of RNNs, specifically the LSTM architecture, in achieving precise predictions within the realm of time series data. The unique design of LSTM networks addresses limitations faced by conventional RNNs, notably the vanishing gradient problem, while simultaneously embracing the ability to capture intricate long-range dependencies inherent in time series data. It's important to recognize that this tutorial serves as an introductory stepping stone towards implementing a fundamental multivariate LSTM framework, and the realm of time series analysis is broad and diverse which often require more nuanced strategies. Nevertheless, this tutorial lays the groundwork for working with multivariate time series data which will enhance your expertise in the domain.
